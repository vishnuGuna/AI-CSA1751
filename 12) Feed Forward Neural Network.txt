import numpy as np

def sigmoid(x):
  """
  This function defines the sigmoid activation function,
  commonly used in hidden layers of neural networks.
  """
  return 1 / (1 + np.exp(-x))

def feedforward_neural_network(X, y, hidden_size, learning_rate, epochs):
  """
  This function trains a feedforward neural network.

  Args:
      X: Training input data (2D array)
      y: Target output data (2D array)
      hidden_size: Number of neurons in the hidden layer
      learning_rate: Step size for weight updates during training
      epochs: Number of training iterations

  Returns:
      W1, b1, W2, b2: Trained weights and biases for hidden and output layers
  """

  # Initialize weights and biases using Python's built-in random module
  input_size, output_size = X.shape[1], y.shape[1]
  W1 = np.random.rand(input_size, hidden_size)
  b1 = np.zeros((1, hidden_size))
  W2 = np.random.rand(hidden_size, output_size)
  b2 = np.zeros((1, output_size))

  # Training loop
  for epoch in range(epochs):
    # Forward propagation
    Z1 = np.dot(X, W1) + b1  # Weighted sum with bias for hidden layer
    A1 = sigmoid(Z1)          # Apply sigmoid activation

    Z2 = np.dot(A1, W2) + b2  # Weighted sum with bias for output layer
    y_pred = sigmoid(Z2)       # Apply sigmoid activation (output prediction)

    # Backpropagation
    dZ2 = y_pred - y           # Calculate error
    dW2 = np.dot(A1.T, dZ2)    # Update weights for output layer using chain rule
    db2 = np.sum(dZ2, axis=0, keepdims=True)  # Update biases for output layer

    dZ1 = np.dot(dZ2, W2.T) * (A1 * (1 - A1))  # Backpropagate error
    dW1 = np.dot(X.T, dZ1)    # Update weights for hidden layer
    db1 = np.sum(dZ1, axis=0, keepdims=True)  # Update biases for hidden layer

    # Update weights and biases with learning rate
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2

  return W1, b1, W2, b2

# Example usage (replace with your own data)
X = np.array([[0, 1], [1, 0], [1, 1], [0, 0]])  # Input data
y = np.array([[0], [1], [1], [0]])  # Target output
hidden_size = 4
learning_rate = 0.1
epochs = 1000

W1, b1, W2, b2 = feedforward_neural_network(X, y, hidden_size, learning_rate, epochs)

# Use the trained network for predictions (replace with new input)
new_input = np.array([[0, 1]])
Z1 = np.dot(new_input, W1) + b1
A1 = sigmoid(Z1)
Z2 = np.dot(A1, W2) + b2
y_pred = sigmoid(Z2)
print(y_pred)
